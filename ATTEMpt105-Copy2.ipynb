{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#    July 03, 2019                                                     #\n",
    "#    11:45                                                             #\n",
    "#    Created by: Ankit Chillar, Kunal Gehlot                           #\n",
    "#    Company: Panda Projects                                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET PATH FOR THE DIRECTORY FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = 'D:\\OneDrive\\Documents\\PANDA\\siim-acr-pneumothorax-segmentation\\sample images\\*.dcm'\n",
    "DATASET_PATH = 'D:\\OneDrive\\Documents\\PANDA\\siim-acr-pneumothorax-segmentation\\dicom-images-train\\*\\*\\*.dcm'\n",
    "TEST_PATH = 'D:\\OneDrive\\Documents\\PANDA\\siim-acr-pneumothorax-segmentation\\dicom-images-test\\*\\*\\*.dcm'\n",
    "RLE_SAMPLE = 'D:/OneDrive/Documents/PANDA/siim-acr-pneumothorax-segmentation/sample images/train-rle-sample.csv'\n",
    "RLE_PATH = 'D:/OneDrive/Documents/PANDA/siim-acr-pneumothorax-segmentation/train-rle.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n",
    "import keras.layers as kl\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict\n",
    "from pydicom.data import get_testdata_files\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, concatenate, MaxPooling2D, Conv2DTranspose, Activation\n",
    "from collections import defaultdict\n",
    "from pydicom.data import get_testdata_files\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.Session()\n",
    "tf.device('/gpu:0')\n",
    "\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='Siren.wav', autoplay=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PixelArray(dataset, figsize=(10,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n",
    "    print(dataset.pixel_array)\n",
    "    print(plt.cm.bone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASK2RLE AND RLE2MASK FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img, width, height):\n",
    "    rle = []\n",
    "    lastColor = 0;\n",
    "    currentPixel = 0;\n",
    "    runStart = -1;\n",
    "    runLength = 0;\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            currentColor = img[x][y]\n",
    "            if currentColor != lastColor:\n",
    "                if currentColor == 255:\n",
    "                    runStart = currentPixel;\n",
    "                    runLength = 1;\n",
    "                else:\n",
    "                    rle.append(str(runStart));\n",
    "                    rle.append(str(runLength));\n",
    "                    runStart = -1;\n",
    "                    runLength = 0;\n",
    "                    currentPixel = 0;\n",
    "            elif runStart > -1:\n",
    "                runLength += 1\n",
    "            lastColor = currentColor;\n",
    "            currentPixel+=1;\n",
    "\n",
    "    return \" \".join(rle)\n",
    "\n",
    "def rle2mask(rle, width, height):\n",
    "    mask= np.zeros(width* height)\n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]\n",
    "    lengths = array[1::2]\n",
    "\n",
    "    current_position = 0\n",
    "    for index, start in enumerate(starts):\n",
    "        current_position += start\n",
    "        mask[current_position:current_position+lengths[index]] = 255\n",
    "        current_position += lengths[index]\n",
    "\n",
    "    return mask.reshape(width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SORTING THE TRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = get_testdata_files('rtplan.dcm')[0]\n",
    "files = sorted(glob(DATASET_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE NO OF FILES IMPORTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ THE TRAIN RLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLEs = pd.read_csv(RLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINTING THE LENGTH OF THE RLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RLEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A LIST TO STORE THE RLE FOR DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLEsL = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE NO OF SAMLES WHICH ARE ANNOATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2379 of 10675 images are annotated (Positive) \n"
     ]
    }
   ],
   "source": [
    "for image_id, rle in zip(RLEs['ImageId'], RLEs[' EncodedPixels']):\n",
    "    RLEsL[image_id].append(rle)\n",
    "annotated = {k: v for k, v in RLEsL.items() if v[0] != ' -1'}\n",
    "print(\"%d of %d images are annotated (Positive) \" % (len(annotated), len(RLEsL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALUES WITH NO VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values are: (DROP THESE) 37\n"
     ]
    }
   ],
   "source": [
    "print('Missing values are: (DROP THESE)', len(files) - len(RLEsL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING BY REMOVING THE ABOVE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in files:\n",
    "    \n",
    "#     print(RLEs.ImageId.str.contains(i.split('\\\\')[-1][:-4]))\n",
    "    if RLEs.ImageId.str.contains(i.split('\\\\')[-1][:-4]).any():\n",
    "        count = count\n",
    "    else:\n",
    "        files.remove(i)\n",
    "        #print(\"False\")\n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR DROPPING THE DUPLICATE FILES FROM THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = RLEs[RLEs.duplicated(subset='ImageId', keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORING THE RLES AND INDICES IN ds1 AND ds2 RESPECTIVELY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = set([tuple(line) for line in RLEs.values])\n",
    "ds2 = set([tuple(line) for line in indices.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINTING THE LENGTH OF THE DATASET AFTER REMOVING FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10675"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds1.difference(ds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLEs = pd.DataFrame(list(ds1.difference(ds2)), columns = ['ImageId', ' EncodedPixels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLEs = RLEs.set_index('ImageId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR VISUALIZING A PART OF THE TRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing the part of the full dataset .........\")\n",
    "\n",
    "start = 20\n",
    "numberOfImages = 50\n",
    "\n",
    "fig, ax = plt.subplots(nrows = numberOfImages, ncols = 1, sharey=True, figsize=(250, 250))\n",
    "for q, path in enumerate(glob(DATASET_PATH)[start:start+numberOfImages]):\n",
    "    dataset = pydicom.dcmread(path)\n",
    "    ax[q].imshow(dataset.pixel_array, cmap = plt.cm.bone)\n",
    "    compare = RLEs[' EncodedPixels'].loc[path.split('\\\\')[-1][:-4]]\n",
    "    print(compare != ' -1')\n",
    "    if compare != ' -1':\n",
    "        mask = rle2mask(RLEs[' EncodedPixels'].loc[path.split('\\\\')[-1][:-4]], 1024, 1024).T\n",
    "        ax[q].set_title('See Marker')\n",
    "        ax[q].imshow(mask, alpha=0.6, cmap='Reds')\n",
    "    else:\n",
    "        ax[q].set_title('Nothing to see')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET THE NO. OF FILES TO BE IMPORTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR IMPORTING THE TRAIN IMAGES AND MASK FROM THE DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND CREATING THE TRAINING DATA X_train AND Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train images and masks ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd0224a21094f85811ab29f836571d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Got'em\n"
     ]
    }
   ],
   "source": [
    "height = 1024\n",
    "width = 1024\n",
    "binSeg = 1\n",
    "\n",
    "X_train = np.zeros((len(files), height, width, binSeg), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(files), height, width, 1), dtype=np.bool)\n",
    "#X_feat = np.zeros((len(files), 1), dtype=np.float32)\n",
    "\n",
    "print('Getting train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "\n",
    "for n, _id in tqdm_notebook(enumerate(files), total=len(files)):\n",
    "    dataset = pydicom.read_file(_id)\n",
    "    \n",
    "    X_train[n] = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "    if '-1' in RLEs.loc[_id.split('\\\\')[-1][:-4],' EncodedPixels']:\n",
    "        Y_train[n] = np.zeros((1024, 1024, 1))\n",
    "    else:\n",
    "        Y_train[n] = np.expand_dims(rle2mask(RLEs.loc[_id.split('\\\\')[-1][:-4],' EncodedPixels'], 1024, 1024), axis=2)\n",
    "print(\"Got'em\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR RESHAPING THE IMAGES TO FEED INTO THE U-NET MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASE HEIGHT AND WIDTH TO DECREASE THE TRAIN SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECREASE HEIGHT AND WIDTH TO INCREASE THE TRAIN SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE TRAINING SAMPLES MEANS LESS LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 64\n",
    "width = 64\n",
    "X_train = X_train.reshape((-1, height, width, 1))\n",
    "Y_train = Y_train.reshape((-1, height, width, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE COEF FUNCTION FOR PIXEL COMPARSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASE SMOOTH FOR GETTING THE DICE COEF CLOSE TO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECREASE SMOOTH FOR GETTING THE DICE COEF CLOSE TO 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    print(binSeg, 'in dice')\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE CONVOLUTIONAL LAYERS (CL) CAN BE ADDED TO MODEL TO INCREASE THE TOTAL NO OF PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE NO OF PARAMETERS DECREASES THE LOSS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET MODEL (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((height, width, binSeg))\n",
    "\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n",
    "c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = MaxPooling2D((2, 2)) (c4)\n",
    "\n",
    "#c41 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "#c41 = Conv2D(64, (3, 3), activation='relu', padding='same') (c41)\n",
    "#p41 = MaxPooling2D((2, 2)) (c41)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "#u61 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "#u61 = concatenate([u61, c41])\n",
    "#c61 = Conv2D(64, (3, 3), activation='relu', padding='same') (u61)\n",
    "#c61 = Conv2D(64, (3, 3), activation='relu', padding='same') (c61)\n",
    "\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "#c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "#c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "#c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "#c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=[dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALLING model.fit FOR CHECKING THE VALUE OF LOSS AND DICE COEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MAKING THE model.h5 FOR FUTURE TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECREASE THE batch_size TO IMPROVING THE ACCURACY OF THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCREASE THE batch_size TO REDUCING THE PROCESSING TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "results = model.fit(X_train, Y_train, validation_split=.2, callbacks=callbacks, batch_size=25, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, Y_train, validation_split=.2, batch_size=50, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = sorted(glob(TEST_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE LENGTH OF THE TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESIZE IMAGES TO MAKE SURE ALL IMAGES ARE OF SAME SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 1024\n",
    "width = 1024\n",
    "binSeg = 1\n",
    "\n",
    "X_test = np.zeros((len(test_files), height, width, binSeg), dtype=np.uint8)\n",
    "\n",
    "for n, _id in tqdm_notebook(enumerate(test_files), total=len(test_files)):\n",
    "    dataset = pydicom.read_file(_id)\n",
    "    \n",
    "    X_test[n] = np.expand_dims(dataset.pixel_array, axis=2)\n",
    "    \n",
    "X_test = X_test.reshape((-1, 64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR VISUALIZING THE PART OF THE TEST DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing the part of the test dataset .........\")\n",
    "\n",
    "start = 20\n",
    "numberOfImages = 50\n",
    "\n",
    "fig, ax = plt.subplots(nrows = numberOfImages, ncols = 1, sharey=True, figsize=(250, 250))\n",
    "for q, path in enumerate(glob(TEST_PATH)[start:start+numberOfImages]):\n",
    "    testdata = pydicom.dcmread(path)\n",
    "    ax[q].imshow(testdata.pixel_array, cmap = plt.cm.bone)\n",
    "    ax[q].set_title('Images')\n",
    "    #print(path)\n",
    "    #mask = rle2mask(RLEs[' EncodedPixels'].loc[path.split('\\\\')[-1][:-4]], 1024, 1024).T\n",
    "    #ax[q].imshow(mask, alpha=0.6, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD THE MODEL FOR PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on validation set (this must be equals to the best log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate({'img': X_valid, 'feat': X_feat_valid}, y_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_train = model.predict({'img': X_train, 'feat': X_feat_train}, verbose=1)\n",
    "#preds_val = model.predict({'img': X_valid, 'feat': X_feat_valid}, verbose=1)\n",
    "#preds_test = model.predict({'img': X_test, 'feat': X_feat_test}, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
